{% extends "layout.html" %}

{% block content %}
    <h2>Api</h2>

    <p>We offer a GRPC based Api. You can generate your client for your prefered language using the generators provided.</p>
    <p>You can reach the public endpoint here: <code>challenge.msrg.in.tum.de:5023</code> </p>
    <p>You can download the protos to generate the client here: <a href="/static/debs.proto">Proto</a></p>
    <p>See <a href="https://grpc.io/">grpc.io</a> for tutorials.</p>

    <h2>Usage</h2>
    <p>The illustration below shows the API interactions. Next we give more details to the steps and examples in Python code. Questions/Errors/Problems? => christoph.doblander@in.tum.de</p>
    <img src="/static/img/debs.png" class="img-fluid" alt="Api usage">

    <h4>Step 1</h4>
    <p>First, get the location data. This returns a message which contains a list of Locations. The message is bigger then what the default configuration allows. Hence increase the max_receive_message_length</p>
    <pre>
        <code>
op = [('grpc.max_send_message_length', 10 * 1024 * 1024),
      ('grpc.max_receive_message_length', 100 * 1024 * 1024)]
with grpc.insecure_channel('challenge.msrg.in.tum.de:5023', options=op) as channel:
    stub = api.ChallengerStub(channel)
    loc = stub.getLocations(empty_pb2.Empty()) #get all the locations
        </code>
    </pre>

    <h4>Step 2</h4>
    <p>Create a new Benchmark. You have to give it your token (see profile), set a benchmark name (this is used for your personal statistics) and the batchsize.
        The event rate is around 4000 messages/second. For the final evaluation you will get access to VMs in our datacenter and bandwidth shouldn't be the bottleneck.</p>
    <pre>
        <code>
benchmarkconfiguration = ch.BenchmarkConfiguration(token="checkyourprofile",
                                                   batch_size=5000,
                                                   benchmark_name="shows_up_in_dashboard")
bench = stub.createNewBenchmark(benchmarkconfiguration)
        </code>
    </pre>

    <h4>Step 3 (optional)</h4>
    <p>Depending on your connectivity you have a latency and throughput. Optionally, we try to account for this by first measuring it.
        The payload of a Ping corresponds roughly to the payload of a batch and the returning Pong roughly the payload of a Result
        This kind of measurement is just for development and experimentation (since, well, it could be easily cheated ;-))
        We do not consider that once you deploy your implementation on the VMs in our infrastructure.</p>
    <pre>
        <code>
ping = stub.initializeLatencyMeasuring(benchmark)
for i in range(10):
    ping =stub.measure(ping)
stub.endMeasurement(ping)
        </code>
    </pre>

    <h4>Step 4</h4>
    <p></p>

    <pre>
        <code>
batch = self.challengerstub.nextMessage(bench)
while batch:
    result_payload = processTheBatch(batch) #here is your implementation ;)
    result = ch.Result(benchmark_id=bench.id, #The id of the benchmark
                       payload_seq_id=batch.seq_id,
                       result=result_payload)

    stub.processed(result) #send the result
    if batch.last:
        break

    batch = self.challengerstub.nextMessage(bench)
        </code>
    </pre>

{% endblock %}