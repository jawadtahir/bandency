{% extends "layout.html" %}

{% block content %}
    <h2>Api</h2>

    <p>We offer a GRPC based Api. You can generate your client for your preferred language using the generators provided.</p>
    <p>The public API endpoint can be reached here: <code>challenge.msrg.in.tum.de:5023</code> </p>
    <p>You can download the protos to generate the client here: <a href="/static/challenger.proto">Proto</a></p>
    <p>See <a href="https://grpc.io/">grpc.io</a> for documentation and tutorials.</p>

    <h2>Usage</h2>
    <p>The illustration below shows the API interactions. Next we give more details to the steps and examples in Python code.</p>
    <img src="/static/img/debs.png" class="img-fluid" alt="Api usage">
    <p>You can download the example code shown below here: <a href="/static/example.py">example.py</a></p>

    <h4>Step 0</h4>
    <p>Generate the client code, as example, we use Python. For other languages check the documentation</p>

    <pre>
    <code>
# Install dependencies
pip install grpcio

# Generate client code
python -m grpc_tools.protoc -I . --python_out=. --grpc_python_out=. challenger.proto
    </code>
    </pre>

    <h4>Step 0</h4>
    <p>Initilize the client stub from the generated code</p>
    <pre>
        <code>
import challenger_pb2 as ch
import challenger_pb2_grpc as api

op = [('grpc.max_send_message_length', 10 * 1024 * 1024),
      ('grpc.max_receive_message_length', 100 * 1024 * 1024)]
with grpc.insecure_channel('challenge.msrg.in.tum.de:5023', options=op) as channel:
    stub = api.ChallengerStub(channel)
        </code>
    </pre>

    <h4>Step 1</h4>
    <p>Create a new Benchmark. You have set your token (see profile), set a benchmark name (this is only shown in your statistics) and the batchsize.
        The benchmark_type should be set to "test" if you experiment.
        Also add the list of which queries you want to run, either just one, [Q1] or [Q2] or both at the same time [Q1, Q2].
        The real event rate of the source stream is around 4000 messages/second. The maximum batch size allowed is 20k.</p>
    <p>For the final evaluation we will post the specific benchmark configuration.</p>
    <pre>
        <code>
benchmarkconfiguration = ch.BenchmarkConfiguration(token="checkyourprofile",
                                                   batch_size=5000,
                                                   benchmark_name="shows_up_in_dashboard",
                                                   benchmark_type="test",
                                                   queries=[ch.BenchmarkConfiguration.Query.Q1, ch.BenchmarkConfiguration.Query.Q2])
bench = stub.createNewBenchmark(benchmarkconfiguration)
        </code>
    </pre>

    <h4>Step 2</h4>
    <p>Then, get the location data. The location data depends on the benchmark_type. The message can be bigger then the gRPC default configuration allows. Hence increase the max_receive_message_length.</p>
    <pre>
        <code>
    loc = stub.getLocations(empty_pb2.Empty()) #get all the locations
        </code>
    </pre>


    <h4>Step 3 (optional)</h4>
    <p>Depending on your connectivity you might experience different latency and throughput. Optionally, we try to account for this by first measuring it.
        The payload of a Ping corresponds roughly to the payload of a batch and the returning Pong roughly the payload of a Result
        This kind of measurement is just for development and experimentation (since, well, it could be easily cheated ;-), how is a thought exercise for the participant).
        We do not consider these measurements once you deploy your implementation on the VMs in our infrastructure.
        The final evaluation will be run on VMs in our datacenter which assures level playing field.</p>
    <pre>
        <code>
ping = stub.initializeLatencyMeasuring(benchmark)
for i in range(10): #to get a reasonable average
    ping = stub.measure(ping)
stub.endMeasurement(ping)
        </code>
    </pre>

    <h4>Step 4</h4>
    <p>First start the Benchmark. This sets the timestamp server side for the throughput measurments. Then process all the batches. The batches are correlated for the latency measurements.</p>
    <p>Once you called the endBenchmark RPC, we calculate the results. For testing, you can call endBenchmark early.</p>

    <pre>
        <code>
stub.startBenchmark(benchmark)

batch = stub.nextBatch(benchmark)
while batch:
    result_payload_q1 = processTheBatchQ1(batch) #here is your implementation ;)
    result_payload_q1 = ch.ResultQ1Payload(resultData=1)
    result = ch.ResultQ1(benchmark_id=benchmark.id,  #The id of the benchmark
                         payload_seq_id=batch.seq_id,
                         result=result_payload_q1)

    stub.resultQ1(result)  # send the result of query 1, also send the result of Q2 in case you calculate both

    # do the same for Q2
    stub.resultQ2(result of query2)

    if batch.last:
        break

    batch = stub.nextMessage(benchmark)

stub.endBenchmark(benchmark)
        </code>
    </pre>

    <p>Still have questions/Errors/Problems? => debschallenge2021@gmail.com</p>

{% endblock %}